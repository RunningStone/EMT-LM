{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_loc = \"/home/shi/WorkSpace/projects/scMultiNet_workspace/\"\n",
    "root = \"/home/shi/WorkSpace/projects/scMultiNet_Data/\"\n",
    "save_root = root + \"/Step_1_data/Dataset_netrin_ST/\"\n",
    "import sys\n",
    "sys.path.append(code_loc ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with a vocab list\n",
    "# this related with the gene2vec model\n",
    "#----> pre-trained part \n",
    "vocab_loc = code_loc +\"/support_data/vocab_gene2vec_16906.pkl\"\n",
    "target_label = 'Ground_Truth' # the label to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_path = root+\"/Step_0_data/netrin_spatial_expression_data_filtered_unnormalised.csv\" \n",
    "data_path = root +  \"/Step_0_data/2024_03_13_EMT_netrin_ST.h5ad\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SAMD11</th>\n",
       "      <th>NOC2L</th>\n",
       "      <th>KLHL17</th>\n",
       "      <th>PLEKHN1</th>\n",
       "      <th>HES4</th>\n",
       "      <th>ISG15</th>\n",
       "      <th>AGRN</th>\n",
       "      <th>C1orf159</th>\n",
       "      <th>TNFRSF18</th>\n",
       "      <th>...</th>\n",
       "      <th>DKC1</th>\n",
       "      <th>MPP1</th>\n",
       "      <th>FUNDC2</th>\n",
       "      <th>CMC4</th>\n",
       "      <th>MTCP1</th>\n",
       "      <th>VBP1</th>\n",
       "      <th>CLIC2</th>\n",
       "      <th>TMLHE</th>\n",
       "      <th>SPRY3</th>\n",
       "      <th>VAMP7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAACTGCTGGCTCCAA-1_0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAAGGGATGTAGCAAG-1_0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AACCTCGCTTTAGCCC-1_0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AACGCGGTCTCCAGCC-1_0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AACGTGCGAAAGTCTC-1_0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2618</th>\n",
       "      <td>TTGTTAGCAAATTCGA-1_3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2619</th>\n",
       "      <td>TTGTTCAGTGTGCTAC-1_3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2620</th>\n",
       "      <td>TTGTTGTGTGTCAAGA-1_3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2621</th>\n",
       "      <td>TTGTTTCATTAGTCTA-1_3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2622</th>\n",
       "      <td>TTGTTTGTGTAAATTC-1_3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2623 rows × 12135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Unnamed: 0  SAMD11  NOC2L  KLHL17  PLEKHN1  HES4  ISG15  AGRN  \\\n",
       "0     AAACTGCTGGCTCCAA-1_0     0.0    0.0     0.0      0.0   0.0    0.0   0.0   \n",
       "1     AAAGGGATGTAGCAAG-1_0     0.0    0.0     0.0      0.0   0.0    0.0   2.0   \n",
       "2     AACCTCGCTTTAGCCC-1_0     1.0    0.0     0.0      0.0   0.0    0.0   0.0   \n",
       "3     AACGCGGTCTCCAGCC-1_0     0.0    0.0     0.0      0.0   1.0    0.0   0.0   \n",
       "4     AACGTGCGAAAGTCTC-1_0     1.0    0.0     0.0      0.0   0.0    0.0   4.0   \n",
       "...                    ...     ...    ...     ...      ...   ...    ...   ...   \n",
       "2618  TTGTTAGCAAATTCGA-1_3     3.0    5.0     0.0      1.0  11.0    0.0  13.0   \n",
       "2619  TTGTTCAGTGTGCTAC-1_3     0.0    3.0     0.0      0.0  20.0    1.0  13.0   \n",
       "2620  TTGTTGTGTGTCAAGA-1_3     2.0    4.0     0.0      0.0   6.0    4.0  10.0   \n",
       "2621  TTGTTTCATTAGTCTA-1_3     4.0    5.0     2.0      0.0   4.0    1.0   6.0   \n",
       "2622  TTGTTTGTGTAAATTC-1_3     2.0    7.0     1.0      0.0  29.0    4.0  11.0   \n",
       "\n",
       "      C1orf159  TNFRSF18  ...  DKC1  MPP1  FUNDC2  CMC4  MTCP1  VBP1  CLIC2  \\\n",
       "0          0.0       0.0  ...   0.0   0.0     0.0   0.0    0.0   1.0    0.0   \n",
       "1          0.0       0.0  ...   0.0   0.0     0.0   0.0    0.0   0.0    0.0   \n",
       "2          0.0       0.0  ...   0.0   0.0     1.0   0.0    0.0   1.0    0.0   \n",
       "3          0.0       0.0  ...   1.0   0.0     0.0   0.0    0.0   0.0    0.0   \n",
       "4          0.0       0.0  ...   0.0   0.0     0.0   0.0    0.0   1.0    0.0   \n",
       "...        ...       ...  ...   ...   ...     ...   ...    ...   ...    ...   \n",
       "2618       0.0       0.0  ...   2.0   0.0     1.0   0.0    0.0   2.0    0.0   \n",
       "2619       0.0       0.0  ...  11.0   0.0     1.0   6.0    0.0   6.0    0.0   \n",
       "2620       0.0       0.0  ...   3.0   0.0     2.0   6.0    0.0   4.0    0.0   \n",
       "2621       0.0       0.0  ...   2.0   0.0     1.0   2.0    0.0   4.0    0.0   \n",
       "2622       1.0       0.0  ...   9.0   0.0     2.0   2.0    0.0   5.0    0.0   \n",
       "\n",
       "      TMLHE  SPRY3  VAMP7  \n",
       "0       0.0    0.0    0.0  \n",
       "1       1.0    0.0    0.0  \n",
       "2       0.0    0.0    0.0  \n",
       "3       0.0    0.0    1.0  \n",
       "4       0.0    0.0    0.0  \n",
       "...     ...    ...    ...  \n",
       "2618    0.0    0.0    1.0  \n",
       "2619    3.0    0.0    3.0  \n",
       "2620    1.0    2.0    1.0  \n",
       "2621    0.0    0.0    1.0  \n",
       "2622    2.0    0.0    6.0  \n",
       "\n",
       "[2623 rows x 12135 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2623, 12134) 2623 12134\n",
      "False False\n",
      "0.0 1698.0\n",
      "(2623, 12134)\n",
      "AnnData object with n_obs × n_vars = 2623 × 12134\n",
      "    obs: 'Ground_Truth'                       Ground_Truth\n",
      "AAACTGCTGGCTCCAA-1_0             0\n",
      "AAAGGGATGTAGCAAG-1_0             1\n",
      "AACCTCGCTTTAGCCC-1_0             1\n",
      "AACGCGGTCTCCAGCC-1_0             1\n",
      "AACGTGCGAAAGTCTC-1_0             1\n",
      "...                            ...\n",
      "TTGTTAGCAAATTCGA-1_3             0\n",
      "TTGTTCAGTGTGCTAC-1_3             0\n",
      "TTGTTGTGTGTCAAGA-1_3             1\n",
      "TTGTTTCATTAGTCTA-1_3             1\n",
      "TTGTTTGTGTAAATTC-1_3             0\n",
      "\n",
      "[2623 rows x 1 columns] Empty DataFrame\n",
      "Columns: []\n",
      "Index: [SAMD11, NOC2L, KLHL17, PLEKHN1, HES4, ISG15, AGRN, C1orf159, TNFRSF18, TNFRSF4, SDF4, B3GALT6, UBE2J2, SCNN1D, ACAP3, PUSL1, INTS11, CPTP, DVL1, MXRA8, AURKAIP1, CCNL2, ANKRD65, VWA1, ATAD3C, TMEM240, SSU72, FNDC10, MIB2, MMP23B, CDK11B, SLC35E2B, SLC35E2A, NADK, GNB1, CALML6, PRKCZ, FAAP20, SKI, MORN1, RER1, PEX10, PLCH2, PANK4, TNFRSF14, PRXL2B, ARHGEF16, MEGF6, TPRG1L, WRAP73, SMIM1, LRRC47, CEP104, DFFB, C1orf174, AJAP1, NPHP4, KCNAB2, RNF207, ICMT, GPR153, ACOT7, HES2, ESPN, TNFRSF25, PLEKHG5, NOL9, ZBTB48, KLHL21, PHF13, THAP3, DNAJC11, CAMTA1, VAMP3, PER3, PARK7, ERRFI1, SLC45A1, RERE, ENO1, SLC2A5, GPR157, H6PD, SPSB1, TMEM201, PIK3CD, CLSTN1, CTNNBIP1, LZIC, NMNAT1, RBP7, UBE4B, KIF1B, PGD, CENPS, DFFA, PEX14, CASZ1, SRM, EXOSC10, ...]\n",
      "\n",
      "[12134 rows x 0 columns] [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.05889282 0.         0.        ]\n",
      " [0.05889282 0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.11778563 0.23557126 0.         ... 0.05889282 0.11778563 0.05889282]\n",
      " [0.23557126 0.29446408 0.11778563 ... 0.         0.         0.05889282]\n",
      " [0.11778563 0.41224971 0.05889282 ... 0.11778563 0.         0.35335689]]\n"
     ]
    }
   ],
   "source": [
    "sample_id = df.iloc[:,0].values\n",
    "gene_id = df.columns[1:].values\n",
    "data_x = df.iloc[:,1:].values\n",
    "\n",
    "print(data_x.shape, len(sample_id), len(gene_id))\n",
    "\n",
    "import numpy as np\n",
    "# confirm nan and inf in data_x\n",
    "print(np.isnan(data_x).any(), np.isinf(data_x).any())\n",
    "# set nan to 0\n",
    "data_x = np.nan_to_num(data_x)\n",
    "\n",
    "# normalize the data to 0-100\n",
    "# 找到数据的最小值和最大值\n",
    "min_val = np.min(data_x)\n",
    "max_val = np.max(data_x)\n",
    "print(min_val, max_val )\n",
    "# 将数据归一化到 0 到 100 的范围\n",
    "normalized_data_x = 100 * (data_x - min_val) / (max_val - min_val)\n",
    "print(normalized_data_x.shape)\n",
    "# create the adata matrix\n",
    "import anndata\n",
    "adata = anndata.AnnData(X = normalized_data_x, obs = pd.DataFrame(index = sample_id), var = pd.DataFrame(index = gene_id))\n",
    "adata.obs[\"Ground_Truth\"] = np.random.choice([0,1], adata.shape[0])\n",
    "print(adata, adata.obs, adata.var, adata.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adata.write(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 2623 × 12134\n",
      "    obs: 'Ground_Truth'\n",
      "Index(['AAACTGCTGGCTCCAA-1_0', 'AAAGGGATGTAGCAAG-1_0', 'AACCTCGCTTTAGCCC-1_0',\n",
      "       'AACGCGGTCTCCAGCC-1_0', 'AACGTGCGAAAGTCTC-1_0', 'AACTCCAGAGCGTGTT-1_0',\n",
      "       'AAGTGTTTGGAGACGG-1_0', 'AAGTTCACTCCAAGCT-1_0', 'AATGCAACCGGGTACC-1_0',\n",
      "       'ACAAGGAAATCCGCCC-1_0',\n",
      "       ...\n",
      "       'TTGTAATCCGTACTCG-1_3', 'TTGTATCACACAGAAT-1_3', 'TTGTCGTTCAGTTACC-1_3',\n",
      "       'TTGTGGCCCTGACAGT-1_3', 'TTGTGGTAGGAGGGAT-1_3', 'TTGTTAGCAAATTCGA-1_3',\n",
      "       'TTGTTCAGTGTGCTAC-1_3', 'TTGTTGTGTGTCAAGA-1_3', 'TTGTTTCATTAGTCTA-1_3',\n",
      "       'TTGTTTGTGTAAATTC-1_3'],\n",
      "      dtype='object', length=2623)\n",
      "Index(['SAMD11', 'NOC2L', 'KLHL17', 'PLEKHN1', 'HES4', 'ISG15', 'AGRN',\n",
      "       'C1orf159', 'TNFRSF18', 'TNFRSF4',\n",
      "       ...\n",
      "       'DKC1', 'MPP1', 'FUNDC2', 'CMC4', 'MTCP1', 'VBP1', 'CLIC2', 'TMLHE',\n",
      "       'SPRY3', 'VAMP7'],\n",
      "      dtype='object', length=12134)\n",
      " negative values: 0\n"
     ]
    }
   ],
   "source": [
    "# check the data\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "adata = sc.read(data_path)\n",
    "\n",
    "print(adata)\n",
    "print(adata.obs.index)\n",
    "print(adata.var.index)\n",
    "print(f\" negative values: {np.sum(adata.X<0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in adata.X: 0 with shape (2623, 12134)\n"
     ]
    }
   ],
   "source": [
    "# adata.X 有多少nan\n",
    "import numpy as np\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "# 假设 adata.X 是你的稀疏矩阵\n",
    "# 首先，检查它是否为稀疏矩阵\n",
    "if issparse(adata.X):\n",
    "    # 将稀疏矩阵转换为密集格式，然后计算 NaN 值的数量\n",
    "    nan_count = np.isnan(adata.X.toarray()).sum()\n",
    "else:\n",
    "    # 如果 adata.X 不是稀疏矩阵，直接计算 NaN 值的数量\n",
    "    nan_count = np.isnan(adata.X).sum()\n",
    "\n",
    "print(f\"NaN values in adata.X: {nan_count} with shape {adata.X.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset_para(var_idx=None, obs_idx='Ground_Truth', vocab_loc='/home/shi/WorkSpace/projects/scMultiNet_workspace//Experiment/support_data/vocab_16k.json', gene_vocab=None, use_key='X', filter_gene_by_counts=False, filter_cell_by_counts=200, normalize_total=10000.0, result_normed_key='X_normed', log1p=True, result_log1p_key='X_log1p', log1p_base=2, subset_hvg=False, hvg_use_key=None, hvg_flavor='seurat_v3', binning=None, result_binned_key='X_binned', tokenize_name='scBERT', return_pt=True, append_cls=True, include_zero_gene=False, cls_token='<cls>', max_len=16000, pad_token='<pad>', pad_value=-2, cls_appended=True, mask_ratio=0.15, mask_value=-1, preprocessed_loc=None, data_layer_name='X_log1p', label_key='Ground_Truth', batch_label_key=None, cls_nb=5, binarize=None, bins=None, bin_min=None, bin_max=None, save_in_obs=True, auto_map_str_labels=True, map_dict=None, n_splits=1, test_size=None, random_state=2023, shuffle=True, sort_seq_batch=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scLLM.Dataset.paras import Dataset_para\n",
    "# define pre-processing by follow original implementation of scBERT\n",
    "\n",
    "from scLLM.Dataset.paras import Dataset_para\n",
    "# define pre-processing by follow original implementation of scBERT \n",
    "dataset_para_cls = Dataset_para(\n",
    "                            var_idx=None,\n",
    "                            obs_idx=\"Ground_Truth\",\n",
    "                            vocab_loc=code_loc + \"/Experiment/support_data/vocab_16k.json\",\n",
    "                            filter_gene_by_counts=False,\n",
    "                            filter_cell_by_counts=200,\n",
    "                            log1p=True,\n",
    "                            log1p_base=2,\n",
    "\n",
    "                            #\n",
    "                            tokenize_name=\"scBERT\",\n",
    "                            cls_nb=5,\n",
    "                            data_layer_name=\"X_log1p\",\n",
    "                            label_key = target_label,#\"Ground_truth\",#\"Ground_truth\",\n",
    "\n",
    "                            test_size=None,#0.2, #use all data to inference\n",
    "                            binarize=None, # not binarize use original label\n",
    "\n",
    "                            )\n",
    "\n",
    "dataset_para_reg = Dataset_para(\n",
    "        vocab_loc=code_loc +\"/Experiment/support_data/vocab_16k.json\",\n",
    "        var_idx = None,#\"genes.gene_short_name\",\n",
    "        obs_idx=\"pseudotimes\",\n",
    "        filter_gene_by_counts=False,\n",
    "        filter_cell_by_counts=200,\n",
    "        log1p=True,\n",
    "        log1p_base=2,\n",
    "\n",
    "        tokenize_name=\"scBERT\",\n",
    "        cls_nb=1,\n",
    "        data_layer_name=\"X_log1p\",\n",
    "\n",
    "        auto_map_str_labels=False,\n",
    "        label_key = target_label,#\"pseudotimes\",\n",
    "\n",
    "        test_size=None,#0.2, #use all data to inference\n",
    "        binarize=None, # not binarize use original label for regression\n",
    "    )\n",
    "\n",
    "dataset_para = dataset_para_cls#dataset_para_cls\n",
    "print(dataset_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shi/anaconda3/envs/scLLM/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "scLLM - INFO - Initializing preprocessor ...\n",
      "scLLM - INFO - use default vocab from dataset_para\n",
      "scLLM - INFO - load vocab from /home/shi/WorkSpace/projects/scMultiNet_workspace//Experiment/support_data/vocab_16k.json\n",
      "scLLM - INFO - Load data from anndata object.\n",
      "scLLM - DEBUG - In original adata with gene 12134\n",
      "scLLM - DEBUG - In original adata with gene 12134\n",
      "scLLM - DEBUG - processing 0/16906\n",
      "scLLM - DEBUG - processing 2000/16906\n",
      "scLLM - DEBUG - processing 4000/16906\n",
      "scLLM - DEBUG - processing 6000/16906\n",
      "scLLM - DEBUG - processing 8000/16906\n",
      "scLLM - DEBUG - processing 10000/16906\n",
      "scLLM - DEBUG - processing 12000/16906\n",
      "scLLM - DEBUG - processing 14000/16906\n",
      "scLLM - DEBUG - processing 16000/16906\n",
      "scLLM - INFO - create anndata in scLLM format..\n",
      "scLLM - DEBUG - restore anndata in scLLM format..\n",
      "scLLM - INFO - Done.\n",
      "scLLM - INFO - Filtering cells by counts ...\n",
      "scLLM - INFO - Filtered cells: 2532\n",
      "scLLM - INFO - Normalizing total counts ...\n",
      "scLLM - INFO - Log1p transforming ...\n",
      "scLLM - INFO - Preprocessing finished.\n"
     ]
    }
   ],
   "source": [
    "# if this is the first time to run, need this block to init translate=True\n",
    "# init preprocessor\n",
    "from scLLM.Dataset.Reader import scReader\n",
    "data_reader = scReader(dataset_para=dataset_para)\n",
    "# init vocab\n",
    "data_reader.init_vocab()\n",
    "\n",
    "# load data\n",
    "data_reader.load_adata(loc = data_path,translate=True)\n",
    "\n",
    "## preprocess\n",
    "data_reader.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 2532 × 16906\n",
      "    obs: 'Ground_Truth', 'n_counts'\n",
      "    uns: 'log1p'\n",
      "    layers: 'X_normed', 'X_log1p' (2532, 16906)\n"
     ]
    }
   ],
   "source": [
    "print(data_reader.adata, data_reader.adata.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in adata.X: 0\n"
     ]
    }
   ],
   "source": [
    "# get layer X_log1p\n",
    "data = data_reader.adata.layers[\"X_log1p\"] \n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "# set nan to 0\n",
    "if isinstance(data, csr_matrix):\n",
    "    # 使用 CSR 格式的稀疏矩阵可以直接通过 data.data 访问非零元素\n",
    "    data.data[np.isnan(data.data)] = 0\n",
    "    # 注意: 这里没有改变矩阵的稀疏结构，只是将非零元素中的 NaN 值替换为 0\n",
    "    data.data[np.isinf(data.data)] = data.data.max()\n",
    "else:\n",
    "    # 如果 data 不是稀疏矩阵，可以直接使用 numpy 的方法\n",
    "    data[np.isnan(data)] = 0\n",
    "    data[np.isinf(data)] = data.max()\n",
    "\n",
    "if issparse(data):\n",
    "    # 将稀疏矩阵转换为密集格式，然后计算 NaN 值的数量\n",
    "    nan_count = np.isnan(data.toarray()).sum()\n",
    "else:\n",
    "    # 如果 adata.X 不是稀疏矩阵，直接计算 NaN 值的数量\n",
    "    nan_count = np.isnan(data).sum()\n",
    "\n",
    "print(f\"NaN values in adata.X: {nan_count}\")\n",
    "data_reader.adata.layers[\"X_log1p\"]  = data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative values in adata.X: 0\n"
     ]
    }
   ],
   "source": [
    "# data <0 的个数\n",
    "if issparse(data):\n",
    "    # 将稀疏矩阵转换为密集格式，然后计算 NaN 值的数量\n",
    "    nan_count = np.sum(data.toarray()<0)\n",
    "else:\n",
    "    # 如果 adata.X 不是稀疏矩阵，直接计算 NaN 值的数量\n",
    "    nan_count = np.sum(data<0)\n",
    "\n",
    "print(f\"Negative values in adata.X: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2532, 16906)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "label_unique = data_reader.adata.obs[dataset_para.label_key].unique()\n",
    "print(label_unique)\n",
    "label_dict = {'fake_lable_0':0,'fake_lable_1':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set fake label\n",
    "#data_reader.adata.obs[dataset_para.label_key]\n",
    "revert_dict = {v:k for k,v in label_dict.items()}\n",
    "# map to str label\n",
    "data_reader.adata.obs[dataset_para.label_key] = data_reader.adata.obs[dataset_para.label_key].map(revert_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scLLM - INFO - Map string labels to int automatically.\n",
      "scLLM - INFO - Mapping from {'fake_lable_1': 0, 'fake_lable_0': 1}\n",
      "scLLM - INFO - Discritize label Ground_Truth in obs_names\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset size:  2532\n",
      "no valset\n",
      "{'fake_lable_0': 0, 'fake_lable_1': 1}\n",
      "weights:  None\n"
     ]
    }
   ],
   "source": [
    "trainset,valset,weights = data_reader.postprocess()\n",
    "\n",
    "# 输出数据集信息\n",
    "print(\"trainset size: \",len(trainset))\n",
    "print(\"valset size: \",len(valset)) if valset is not None else print(\"no valset\")\n",
    "print(label_dict)\n",
    "print(\"weights: \",weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.sample_id = data_reader.adata.obs.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "# 为trainset 添加其他labels\n",
    "\n",
    "#target_task = f\"/TrVal_dataset_GT_{target_stimulate}_{target_cellline}.pkl\"\n",
    "target_task = f\"/TrVal_dataset_{target_label}.pkl\"\n",
    "loc = save_root + target_task\n",
    "# 保存 trainset 到文件，并关联相应labels\n",
    "with open(loc,\"wb\") as f:\n",
    "    dill.dump([trainset,valset,weights,label_dict],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import issparse\n",
    "# 假设adata和raw_adata是已经加载的Anndata对象\n",
    "\n",
    "# 定义一个函数来计算稀疏矩阵每行的哈希值\n",
    "def hash_sparse_row(row):\n",
    "    # 将稀疏矩阵行转换为密集格式然后计算哈希\n",
    "    return hash(tuple(row.toarray()[0]))\n",
    "\n",
    "data1 = trainset.data #adata.X\n",
    "data2 = data_reader.adata.layers[\"X_log1p\"] #raw_adata.X\n",
    "data2_source = data_reader.adata\n",
    "# 检查adata.X是否为稀疏矩阵，然后相应地计算哈希值\n",
    "if issparse(data1):\n",
    "    adata_hashes = [hash_sparse_row(data1[row,:]) for row in range(data1.shape[0])]\n",
    "else:\n",
    "    adata_hashes = [hash(tuple(row)) for row in adata.X.toarray()]\n",
    "\n",
    "# 同样的处理对raw_adata.X进行\n",
    "if issparse(data2):\n",
    "    raw_adata_hashes = [hash_sparse_row(data2[row,:]) for row in range(data2.shape[0])]\n",
    "else:\n",
    "    raw_adata_hashes = [hash(tuple(row)) for row in data2.toarray()]\n",
    "\n",
    "# 创建从raw_adata的行哈希值到其obs.index的映射\n",
    "hash_to_index_map = {hash_: index for hash_, index in zip(raw_adata_hashes, data2_source.obs.index)}\n",
    "\n",
    "\n",
    "# 创建一个列表来存储对应的raw_adata.obs.index\n",
    "mapped_indices = []\n",
    "\n",
    "# 对于adata中的每一行，找到对应的raw_adata.obs.index\n",
    "for i,hash_ in enumerate(adata_hashes):\n",
    "    mapped_index = hash_to_index_map.get(hash_, None)\n",
    "    if mapped_index is not None:\n",
    "        mapped_indices.append(mapped_index)\n",
    "    else:\n",
    "        # 如果找不到对应，可能需要处理这种情况\n",
    "        raise ValueError(f\"No matching index {i} found for hash: {str(hash_)}\" )\n",
    "\n",
    "# 将mapped_indices转换为Pandas Series对象，以便更方便地处理\n",
    "mapped_series = pd.Series(mapped_indices, index=adata.obs.index)\n",
    "\n",
    "# 打印或者返回映射结果\n",
    "print(mapped_series)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
